[
["index.html", "Uncertainty Validation 1 Abstract", " Uncertainty Validation Mark Hagemann 2019-05-03 1 Abstract "],
["introduction.html", "2 Introduction", " 2 Introduction Opening Introduce objectives of SWOT mission, river data products Previous and ongoing validation work Reasons and methods for uncertainty estimation Sources of uncertainty Models of uncertainty using SWOT-observable information While validation of SWOT estimates has begun using simulated and AirSWOT data, the uncertainty component of these estimates remains largely unverified. Validation is challenging for several reasons. Uncertainty is accumulated from a multitude of sources including…, and assessing each component’s contribution to overall error is difficult. Errors are dependent on a variety of factors that vary within and between rivers on a range of temporal and spatial scales, requiring validation data that are both finely detailed and wide-ranging (encompassing a variety of SWOT rivers). In order to be useful for refining uncertainty estimates, a validation of uncertainty models should be predictive of error magnitude using only a set of SWOT-observable or a-priori variables and applicable to cases outside of those used in the validation study. In this study we present a validation of SWOT measurement uncertainty at the river node and reach scales. The products of this validation are threefold. First, we quantify the degree to which empirical distributions of measurement errors match modeled uncertainty across all validation data. Second, we describe and quantify discrepancies between empirical and theoretical error magnitude as a function of several characteristics including number of pixels/nodes, viewing geometry, and layover characteristics. Finally, we develop, calibrate, and validate a model for refining a priori uncertainty estimates using only information available from SWOT and a priori measurements. For the purposes of validation “truth” we employ simulated SWOT observations from several intensively modeled rivers spanning a range of hydraulic regimes, flow geometry, and catchment topography. We conclude with a prognosis for scaling up this validation as more data–simulated and real–become available. We envision this validation framework–particularly the model–being deployed as an additional postprocessing layer used to empirically scale uncertainty estimates within the SWOT river data products. "],
["methods.html", "3 Methods 3.1 SWOT River Data Products 3.2 Datasets 3.3 Error scaling", " 3 Methods 3.1 SWOT River Data Products SLC product, simulation thereof Stages of processing, fields calculated Pixel cloud node assignment reach aggregation computation of slope Prior database, uses Required to assign pixels Refinement of prior db 3.2 Datasets The study area consists of 733 nodes in comprising 10 reaches in the Sacramento River between 38.92 and 39.75 degrees latitude. Two simulations of the SWOT single-look complex (SLC) were created using two different flow conditions; other simulation parameters including \\(\\sigma_0\\) for water and land, SWOT pass number, and smearing distance were held constand. The water and land \\(\\sigma_0\\) were selected so as to remove any impact of layover. Datasets come from simulation of SWOT SLC using hydrodynamic models forced by observed hydrologic conditions Validation data come from GDEM Simulated river is #### km section of Sacramento River Passes of SWOT satellite pass n_nodes date flow flow_pctile 249 698 2009-02-20 9224 67.3 264 447 2009-01-10 4513 5.0 527 733 2009-01-19 4203 2.6 Statistics about validation variables 3.3 Error scaling The different SWOT observations are reported with estimates of \\(1 \\sigma\\) uncertainty; the objective of uncertainty validation is to determine how accurately these uncertainty estimates match the behavior of empirical errors. In order to easily compare across a set of validation data with varying uncertainty estimates, we employ a simple transformation such that the mean and variance should be equal across all transformed validation data. Consider an arbitrary measured variable (e.g. node height, reach width, pixel latitude), denoted \\(x_{st}\\), where \\(s\\) indexes location (in space), and \\(t\\) indexes time. Because \\(x\\) is not precisely determined and has nonzero error, we can consider it a random variable. The corresponding true value, of which \\(x\\) is an estimate, is denoted \\(x^*\\). The error, \\(\\epsilon\\), is defined as \\(\\epsilon = x - x^*\\). Because \\(\\epsilon\\) is a transformation of a random variable, it is itself a random variable. We denote the mean, variance, and standard deviation of \\(\\epsilon\\) as \\(\\mu_\\epsilon\\), \\(\\sigma^2_\\epsilon\\), and \\(\\sigma_\\epsilon\\), respectively, and similarly for other random variables. The objective of this study is to validate the quantification of uncertainty, where uncertainty is expressed as an estimate of \\(\\sigma_x\\). We denote this estimate \\(\\hat{\\sigma}_x\\), to distinguish it from the true value \\(\\sigma_x\\). We perform this validation over a potentially large number of locations \\(s\\) and times \\(t\\) (although at present only a single time has been considered). The data for a given observed variable therefore consist of observations of \\(x_{st}\\) and \\(\\hat{\\sigma}_{st}\\); these are both provided in the rivertile product produced by RiverObs. In order to validate the estimates \\(\\hat{\\sigma}_{st}\\), we rely on analogous gdem-derived synthetic data that give, for our purposes, “true” values \\(x^*_{st}\\). Thus we can calculate a corresponding set of empirical errors \\(\\epsilon_{st}\\) for every location and time in the dataset. Since \\(\\epsilon_{st} = x_{st} - x^*_{st}\\), then if our uncertainty estimates are correct (\\(\\hat{\\sigma}_{x_{st}} = \\sigma_{x_{st}}\\)), we obtain \\(\\sigma_{\\epsilon_{st}} = \\sigma_{x_{st}}= \\hat{\\sigma}_{x_{st}}\\). A simple elementwise scaling, \\(e_{st} \\equiv \\epsilon_{st} / \\hat{\\sigma}_{x_{st}}\\) therefore has \\(\\sigma_{e_{st}} = 1\\) for all \\(s\\) and \\(t\\). (Recall that for any random variable \\(Y\\) with mean \\(\\mu_Y\\) and variance \\(\\sigma^2_Y\\), a linear transformation of the form \\(W = aY + b\\) has mean \\(\\mu_W = a\\mu_Y + b\\) and variance \\(a^2\\sigma_Y^2\\). This is true regardless of distribution.) Thus scaled, the errors \\(e_{st}\\) can be compared across locations and times in order to validate the uncertainty estimates. A “good” model for \\(\\sigma_x\\) will result in a set of \\(e_{st}\\) with empirical standard deviation that is close to 1. This observation leads directly to explicit tests for verifying the model responsible for producing \\(\\hat{\\sigma}_{x_{st}}\\). 3.3.1 Error Decomposition Uncertainty can be parsed into two components–bias and variance, corresponding to “systematic” and “random” errors. Strictly speaking, if \\(\\sigma_x\\) is defined to be a standard deviation (square root of variance), then it does not include bias, and therefore ignores a potentially large component of uncertainty. Adjusting \\(\\hat{\\sigma}_x\\) cannot remove this bias, but it can account for it by “lumping it in” to the variance. This choice of whether or not to account for bias leads to two separate sets of uncertainty validation–with and without bias-adjusting the scaled errors. Since the uncertainty estimates are produced using a method that explicitly assumes zero bias, it makes sense to validate these estimates without including bias. On the other hand, real-world errors will include both random and systematic components, and a true assessment of uncertainty should somehow measure how well uncertainty estimates account for the full error distribution–including bias. 3.3.2 Assumptions of Normality The uncertainty estimates \\(\\sigma_{x_{st}}\\) are provided without explicitly assuming a probability distribution for \\(x\\) (or, equivalently, \\(\\epsilon\\)). However, explicit probabilistic tests for the truth of \\(\\sigma_{x}\\) are parametric, and rely on distributional assumptions, for example normality. The normality assumption is a reasonable one in the case of SWOT data, for at least two reasons. First, the feature-level measurements constitute aggregating (summing) over many independent radar “looks”, and as such will tend towards normality according to the central limit theorem. Second, the normal distribution has the property that it has the maximum entropy–colloquially, the maximum randomness–of any distribution for which all that is known is the mean and standard deviation. In SWOT data, that is the case: all that is reported is an estimate (\\(x\\)) and a standard deviation (\\(\\sigma_x\\)). Thus, the normal distribution is a conservative choice; using any other distribution would unduly constrain the randomness it expresses. 3.3.3 Tests for validity of \\(\\hat{\\sigma}_{x}\\) 3.3.3.1 Visual Checks The scaled errors, \\(e_{st}\\), are expected to have standard deviation equal to 1 for all locations \\(s\\) and times \\(t\\). Therefore, a histogram of \\(e_{st}\\) should display most scaled errors between -1 and 1, and with a peak at 0 (if errors are unbiased and symmetrical). Other distribution plots (e.g. box and violin plots) should reveal the same. The stronger assumption of normality can be visually inspected using a histogram with a standard normal pdf superimposed; this should roughly match the (appropriately scaled) bars of the histogram. A normal quantile-quantile (QQ) plot checks the normality assumption more explicitly, and makes clear which parts of the distribution most strongly diverge from normality. This rationale does not rule out selection of a more appropriate distribution based on empirical error distributions, rather the point is to justify the normal assumption a priori. 3.3.3.2 Qualitative checks Descriptive statistics including the empirical standard deviation, RMSE, and percentiles of scaled errors can verify qualitatively the closeness of the observed to the theoretical error distribution. If the errors are unbiased and uncertainty estimates are correct, then both the RMSE and standard deviation should be approximately 1. Similarly, if we assume normally distributed errors then empirical quantiles should match theoretical–for example, the .025, .16, .5, .84, and .975 quantiles of \\(e_{st}\\) should be approximately -1.96. -1, 0, 1, and 1.96, respectively. We do not expect these correspondences to be exact, only close enough to satisfy intuition. 3.3.3.3 Quantitative Hypothesis Test Assuming errors to be normally distributed, we can formulate an explicit hypothesis test for the following: Null hypothesis: \\(\\sigma_{e_{st}} = 1\\) for all \\(s\\), \\(t\\). Alternative hypothesis: \\(\\sigma_{e_{st}} \\ne 1\\) for some \\(s\\) and/or \\(t\\). Under the null hypothesis, a test statistic of the form \\(\\chi^2 \\equiv \\sum_{s,t}(e_{st} - \\bar{e})^2\\) has a chi-square distribution with degrees of freedom equal to the total number of data in the validation set minus 1. Using a type-1 error rate of 0.05, the hypothesis test would reject the null hypothesis if the test statistic lies outside of the 0.025 - 0.975 quantile range of the chi-square distribution, and otherwise not reject the null hypothesis. The same test could be modified to include the assumption that errors are unbiased, by removing the subtraction of \\(\\bar{e}\\) from the definition of \\(\\chi^2\\) above. "],
["results.html", "4 Results 4.1 Node Results 4.2 Reach results 4.3 Part 2: Factors affecting errors and uncertainty", " 4 Results The results are presented in 2 parts. Part 1 considers all validation data as a single set over which uncertainty estimates are evaluated. It seeks a simple yes/no answer to the question of whether predicted uncertainty matches empirical errors across the entire validation set. Phases 2 and separates results depending on variables such as uncertainty magnitude and feature size and seeks out relationships between these variables and error magnitude. 4.1 Node Results Node variables validated include height, height2, width, and total area. The “height2” variable differs from “height” only in its uncertainty estimate (using a different method to estimate uncertainty). The results in phase 1 are presented in 3 forms: Chi-square hypothesis tests of empirical variance, plots comparing theoretical to empirical distributions, and tables comparing theoretical to empirical statistics. Results at the node scale are presented in section ####; reach-scale results are presented in section ####. 4.1.1 Hypothesis tests In the combined validation data from all 3 passes, 2-sided chi-square hypothesis tests rejected the null hypothesis (\\(\\sigma = \\hat{\\sigma}\\)) for all node variables at a significance level of 0.05. Even when errors were bias-adjusted the results of the hypothesis test are the same (Table ####). This indicates that empirical variance was significantly different from modeled variance. However, the hypothesis test for height was only marginally significant (\\(p = 0.035\\) and \\(0.043\\) for non-adjusted and bias-adjusted, respectively), whereas all other variables had highly significant hypothesis tests (\\(p &lt; 10^{-6}\\)). variable teststat df pval pval_debias area_total 14840.043 1877 0 0.0000e+00 height 2256.927 1877 0 3.7398e-03 height2 3590.144 1877 0 0.0000e+00 width 14840.042 1877 0 0.0000e+00 Restricting the validation to include only the lower-flow conditions (days 110 and 119) gives a different result for height; here the test does not reject the null hypthesis \\(\\sigma = \\hat{\\sigma}\\) at a significance level of 0.05 when bias is removed. All other variables have error variance that is again significantly different from the estimated variance. variable teststat df pval pval_debias area_total 8542.081 1179 0e+00 0.0000000 height 1439.903 1179 5e-07 0.3840719 height2 2531.810 1179 0e+00 0.0000000 width 8542.081 1179 0e+00 0.0000000 4.1.2 Distribution plots Histograms for node-level scaled errors (Fig. ####) illustrate the difference between the empirical (histogram) and theoretical (smooth curve) error distributions. Area and width (a nodewise constant scaling of area) have heavy upper tails, especially for the high-flow day 220, indicating a tendency to vastly overestimate node areas in some cases. Three of the worst overestimates (nodes 286, 301, and 360) are mapped below, and their aggregation of pixels compared to the validation truth in figure ####. Normal quantile-quantile plots (Fig. ####) illustrate the deviation of emprical scaled errors from the theoretical \\(N(0, 1)\\) distribution (solid black line). While the height errors closely match the theoretical distribution for day 109, day 220 slightly but consistently overestimates large height errors, enough to cause the hypothesis test to reject the equality of theoretical and empirical variance. Height2 scaled errors are apparently Gaussian (falling on a straight line in the QQ plot), but have a larger variance than is predicted (slope of a line through the scaled errors is greater than 1). Width and area scaled errors diverge considerably from the theoretical distribution, and the behavior is markedly different between days 109 and 220. Both days’ scaled errors diverge from the theoretical \\(N(0,1)\\) line, but whereas Day 220 has a heavy upper tail, day 109 appears to be Gaussian, but with a larger variance than predicted. 4.1.3 Tables Coverage rates for various confidence intervals (Table ####) again reflect the underestimation of variance in the uncertainty models. Only the 99% confidence interval for height contains the expected amount (99%) of the data; all other intervals contain less than the theoretical amount. ci68 ci90 ci95 ci99 area_total 34.3 55.1 61.1 73.7 height 65.5 87.1 93.2 97.8 height2 53.7 77.6 85.8 93.7 width 34.3 55.1 61.1 73.7 Summary statistics (Table ####) indicate exactly by how much empirical statistics (bias, standard deviation, RMSE), differ from theoretical values (0, 1, and 1, respectively). Area and width \\(\\hat{\\sigma}\\)’s would need to be multiplied by a factor of 2.36 in order to match the theoretical variance (including bias), whereas \\(\\hat{\\sigma}\\) for height and height2 would require smaller adjustments. variable bias sd rmse area_total 1.30 2.49 2.81 height 0.32 1.05 1.10 height2 0.43 1.32 1.38 width 1.30 2.49 2.81 4.2 Reach results variable teststat df pval pval_debias area_total 8583.35 28 0.00 0.00 height 162.64 28 0.00 0.00 slope 36.74 28 0.25 0.41 width 9199.21 28 0.00 0.00 4.3 Part 2: Factors affecting errors and uncertainty Uncertainty estimates are a function of variables including number of pixels/nodes position in cross-track "]
]
