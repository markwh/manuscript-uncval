# Introduction

The Surface Water and Ocean Topography (SWOT, [@fu2012swot]) mission will join the fleet of earth observation satellites upon its launch in 2021. Thereafter it will begin its mission of monitoring the world's oceans, lakes, reservoirs, wetlands, and rivers, providing some #### GB per day of information to the scientific community. This flow of data will be the first of its kind, involving instrumentation and data processing pipelines never before deployed. The termini of these data streams will be a variety of data products whose veracity and reliability are essential to the pursuit and application of hydrologic science.

The hydrology (**S**urface **W**ater) data measured by SWOT--including width, slope, and elevation relative to geoid on rivers whose average width is over 100m. This information will become publically accessible datasets with expected applications including discharge estimation in ungaged basins [@durand2016; @gleason2017], data assimilation with hydraulic models [@andreadis2018; @oubanas2018], ####

River measurements will be aggregated to data products at multiple spatial scales to produce estimates of river height, with, spatial extent, surface slope, and discharge. Because these measurements are often combined probabilistically with other data sources--e.g. prior distributions in Bayesian discharge estimation [@durand2014; @hagemann2017], or hydrodynamic models in data assimilation [@andreadis2018; @oubanas2018]--SWOT data products will contain uncertainty estimates associated with measured variables. The measurements and the associated uncertainty estimates are derived from physics-based equations that relate the quantities of interest to known and directly measured quantities including satellite position, radar range, power, and interferometric phase. 

Validation of SWOT data products and underlying processing pipelines is an ongoing effort, the culmination of which will come only after the satellite is in orbit. The first 3 months of the SWOT mission will be devoted to instrument calibration and processing validation (cal/val), employing a fast-sampling orbit wherein the orbit time for selected cal/val sites will be ~1 day [@swot2018calval]. This sampling will coincide with field surveys generating in-situ comparison datasets. In order to benchmark instrument performance prior to launch, an airborne instrument, termed AirSWOT, was developed to produce data analogous to the SWOT satellite [@altenau2017; @tuozzolo2019; @pitcher2019]. While this effort was successful in demonstrating the ability for SWOT to measure slope and water surface elevation, the many differences between the airborne and spaceborne missions (e.g. look angle, swath size) preclude this data from being used to validate of uncertainty models, which to date remain largely unverified. 

Meanwhile, verification of data processing flows has been proceeding using simulated data products generated from modeled terrain, hydrology, satellite trajectory, and radar physics. The most recent generation of simulations include estimates of uncertainty accompanying simulated measurements. 


In this study, we use simulated data products to validate river measurements against synthetic truth data in order to assess how well empirical (albeit simulated) errors comport with model-encoded expectations. Our purpose is not to investigate measurement errors directly--that is, how measured values differ from truth--but instead to determine how these errors behave vis a vis the measurements' accompanying uncertainty estimates. Thus instead of questioning how errors behave across a range of validation characteristics, we seek to answer how the behavior of errors *relative to estimated uncertainty* differs across the validation axes. In this way we validate both the measurements and their corresponding uncertainty estimates. By validating across a range of processing configurations, hydrologic conditions, and radar look geometries we interrogate the uncertainty models in great detail, including the following questions: How do (dis)agreements between modeled and empirical uncertainty differ across characteristics (look angle, land/water reflectance, channel geometry)? How do errors accumulate from pixel to node to reach scales? What phenomena are responsible for the largest errors? What assumptions encoded in uncertainty models are met, and which are violated? What further work is needed to validate and refine uncertainty models? We conclude with a prognosis for scaling up this validation as more data--simulated and real--become available. 


